"""
Script para coletar dados de treinamento de artigos web e imagens/vÃ­deos
Extrai informaÃ§Ãµes sobre poses corretas e incorretas de fontes online
"""
import requests
from bs4 import BeautifulSoup
import json
import re
from pathlib import Path
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import time


class WebScraper:
    """Coleta informaÃ§Ãµes sobre poses de fisiculturismo de artigos web"""
    
    def __init__(self, output_dir="data_collected/web"):
        """
        Inicializa o scraper
        
        Args:
            output_dir: DiretÃ³rio onde salvar dados coletados
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def scrape_article(self, url: str) -> Optional[Dict]:
        """
        Faz scraping de um artigo sobre poses
        
        Args:
            url: URL do artigo
            
        Returns:
            Dict com conteÃºdo extraÃ­do ou None se falhar
        """
        try:
            print(f"ğŸ“„ Coletando: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extrai tÃ­tulo
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else ""
            
            # Extrai conteÃºdo principal
            # Tenta encontrar artigo principal (varia por site)
            article = soup.find('article') or soup.find('main') or soup.find('div', class_=re.compile('content|article|post'))
            if not article:
                article = soup.find('body')
            
            # Remove scripts e styles
            for script in article.find_all(['script', 'style']):
                script.decompose()
            
            # Extrai texto
            text = article.get_text(separator='\n', strip=True) if article else ""
            
            # Extrai imagens
            images = []
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src')
                if src:
                    full_url = urljoin(url, src)
                    images.append({
                        'url': full_url,
                        'alt': img.get('alt', ''),
                        'title': img.get('title', '')
                    })
            
            # Extrai vÃ­deos
            videos = []
            for video in soup.find_all('video'):
                src = video.get('src')
                if src:
                    full_url = urljoin(url, src)
                    videos.append({'url': full_url})
            
            # Procura por menÃ§Ãµes de poses especÃ­ficas
            pose_keywords = {
                'double_biceps': ['double biceps', 'duplo bÃ­ceps', 'front double biceps'],
                'side_chest': ['side chest', 'peito lateral', 'lateral chest'],
                'side_triceps': ['side triceps', 'trÃ­ceps lateral', 'lateral triceps'],
                'most_muscular': ['most muscular', 'mais muscular', 'crab most muscular'],
                'enquadramento': ['framing', 'enquadramento', 'centering']
            }
            
            detected_poses = []
            text_lower = text.lower()
            for pose, keywords in pose_keywords.items():
                for keyword in keywords:
                    if keyword in text_lower:
                        detected_poses.append(pose)
                        break
            
            result = {
                'url': url,
                'title': title_text,
                'text': text[:5000],  # Limita tamanho
                'images': images[:20],  # Limita quantidade
                'videos': videos[:10],
                'detected_poses': list(set(detected_poses)),
                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            print(f"âœ… Coletado: {len(images)} imagens, {len(videos)} vÃ­deos")
            return result
            
        except Exception as e:
            print(f"âŒ Erro ao coletar {url}: {e}")
            return None
    
    def download_image(self, url: str, save_path: Path) -> bool:
        """
        Baixa uma imagem
        
        Args:
            url: URL da imagem
            save_path: Onde salvar
            
        Returns:
            True se sucesso
        """
        try:
            response = self.session.get(url, timeout=10, stream=True)
            response.raise_for_status()
            
            # Verifica se Ã© imagem
            content_type = response.headers.get('content-type', '')
            if not content_type.startswith('image/'):
                return False
            
            save_path.parent.mkdir(parents=True, exist_ok=True)
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return True
        except Exception as e:
            print(f"âš ï¸ Erro ao baixar {url}: {e}")
            return False
    
    def scrape_multiple(self, urls: List[str], download_images: bool = True) -> List[Dict]:
        """
        Faz scraping de mÃºltiplos artigos
        
        Args:
            urls: Lista de URLs
            download_images: Se True, baixa imagens tambÃ©m
            
        Returns:
            Lista de resultados
        """
        results = []
        images_dir = self.output_dir / "images"
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] Processando...")
            result = self.scrape_article(url)
            
            if result:
                results.append(result)
                
                # Baixa imagens se solicitado
                if download_images and result['images']:
                    for j, img_info in enumerate(result['images'][:5]):  # Limita a 5 por artigo
                        img_url = img_info['url']
                        # Tenta inferir extensÃ£o
                        ext = Path(urlparse(img_url).path).suffix or '.jpg'
                        if ext not in ['.jpg', '.jpeg', '.png', '.webp']:
                            ext = '.jpg'
                        
                        img_filename = f"article_{i}_img_{j}{ext}"
                        img_path = images_dir / img_filename
                        
                        if self.download_image(img_url, img_path):
                            result['images'][j]['local_path'] = str(img_path)
                        
                        time.sleep(0.5)  # Evita sobrecarga
                
                time.sleep(1)  # Delay entre requisiÃ§Ãµes
        
        # Salva resultados consolidados
        output_file = self.output_dir / "scraped_articles.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Coletados {len(results)} artigos")
        print(f"ğŸ’¾ Salvo em: {output_file}")
        
        return results


def main():
    """FunÃ§Ã£o principal"""
    print("="*60)
    print("ğŸŒ Coletor de Dados Web para Treinamento")
    print("="*60)
    
    # URLs conhecidas sobre poses de fisiculturismo
    known_urls = [
        "https://barbend.com/news/bodybuilding-poses/",
        # Adicione mais URLs aqui
    ]
    
    print("\nğŸ“‹ URLs para coletar:")
    for url in known_urls:
        print(f"  - {url}")
    
    # Pergunta URLs adicionais
    print("\nğŸ’¡ Deseja adicionar mais URLs? (s/n)")
    choice = input().strip().lower()
    
    urls = known_urls.copy()
    if choice == 's':
        print("Digite URLs (uma por linha, Enter vazio para terminar):")
        while True:
            url = input().strip()
            if not url:
                break
            if url.startswith('http'):
                urls.append(url)
            else:
                print("âš ï¸ URL invÃ¡lida, ignorando...")
    
    # Pergunta se baixa imagens
    print("\nğŸ“¥ Deseja baixar imagens dos artigos? (s/n)")
    download_imgs = input().strip().lower() == 's'
    
    # Executa scraping
    scraper = WebScraper()
    results = scraper.scrape_multiple(urls, download_images=download_imgs)
    
    print(f"\nâœ… ConcluÃ­do! {len(results)} artigos coletados")
    print(f"ğŸ“ Dados salvos em: {scraper.output_dir}")


if __name__ == "__main__":
    main()
